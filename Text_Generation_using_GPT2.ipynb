{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importing Libraries"
      ],
      "metadata": {
        "id": "I2xyWTqGJRBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the tranformer libraries using bleow command\n",
        "\n",
        "# ! pip install transformers"
      ],
      "metadata": {
        "id": "T-6MF6h0Jt6q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tdFLZPs_HsbV"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input sentence\n",
        "\n",
        "input_sequence = \"I don't know about you, but there's only one thing I want to do after a long day of work\""
      ],
      "metadata": {
        "id": "9qzjP1HRLlcY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the large gpt tokenizer & gpt model\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id = tokenizer.eos_token_id)\n",
        "\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
        "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-medium\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "#GPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "#view model parameters\n",
        "GPT2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlW0YGoMKP-J",
        "outputId": "ad88581c-a688-4af9-ca50-3e02c793dcb6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tfgpt2lm_head_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " transformer (TFGPT2MainLay  multiple                  774030080 \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 774030080 (2.88 GB)\n",
            "Trainable params: 774030080 (2.88 GB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for reproducability\n",
        "SEED = 34\n",
        "\n",
        "#maximum number of words in output text\n",
        "MAX_LEN = 70"
      ],
      "metadata": {
        "id": "F-emtAN5NoVt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get deep learning basics\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "qLgWT__rLMf3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Pass (Greedy Search)"
      ],
      "metadata": {
        "id": "DH6lH9WEQm1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(input_sequence, return_tensors = 'tf')\n",
        "\n",
        "# Generate the text until the output length(which include context length) reaches 50\n",
        "greedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKTvNJ67Nj1g",
        "outputId": "7f0d64d4-3ec2-4a7a-afa5-f3b2ed141ab2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work: go to the gym.\n",
            "\n",
            "I'm not talking about the gym that's right next to my house. I'm talking about the gym that's right next to my office.\n",
            "\n",
            "I'm not talking about the gym that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search with N-Gram Penalities"
      ],
      "metadata": {
        "id": "eT11AWW7Qgih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = GPT2.generate(\n",
        "    input_ids,\n",
        "    max_length = MAX_LEN,\n",
        "    num_beams = 5,\n",
        "    no_repeat_ngram_size = 2,\n",
        "    num_return_sequences = 5,\n",
        "    early_stopping = True\n",
        ")\n",
        "\n",
        "print('')\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "\n",
        "# now we have 3 output sequences\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyUFulL2OIaM",
        "outputId": "1bfd6140-88e7-48a8-c009-463c61d24e65"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I mean, it's\n",
            "1: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a girl\n",
            "2: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a woman\n",
            "3: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's about a guy who has a crush on a beautiful\n",
            "4: I don't know about you, but there's only one thing I want to do after a long day of work, and that's to sit down and watch a movie.\"\n",
            "\n",
            "\"I know, I know,\" you say. \"But you're not going to like this one. It's not a good movie. I'm not sure if\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Sampling"
      ],
      "metadata": {
        "id": "p275tggQRB-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids,\n",
        "                             do_sample = True,\n",
        "                             max_length = MAX_LEN,\n",
        "                             top_k = 0,\n",
        "                             temperature = 0.8\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaZCgfQSRGjl",
        "outputId": "318e7071-3282-490a-b895-cd304d7e4adb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work: drink some beer. Ask anyone who says otherwise.\n",
            "\n",
            "If you're like me and you work in an office with windows that can't possibly be broken, I can assure you that you're not alone. The truth is,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### top-k-sampling"
      ],
      "metadata": {
        "id": "XBbLfPR-RHvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample from only top_k most likely words\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids,\n",
        "                             do_sample = True,\n",
        "                             max_length = MAX_LEN,\n",
        "                             top_k = 50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvrvEZSeS2wd",
        "outputId": "d59fe453-ee31-400d-8280-2275cdc39deb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work or school: take a stroll and catch up on my mail. This is not something that can be done in the privacy of my office during normal business hours. My office is right here, and there are a lot of people who would ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-p-sampling"
      ],
      "metadata": {
        "id": "_oNvAaQLTrsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sample only from 80% most likely words\n",
        "sample_output = GPT2.generate(\n",
        "                             input_ids,\n",
        "                             do_sample = True,\n",
        "                             max_length = MAX_LEN,\n",
        "                             top_p = 0.8,\n",
        "                             top_k = 0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN8z_HsIS3py",
        "outputId": "7c612eae-9c7e-4ec8-b766-dda3e1fbf152"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "I don't know about you, but there's only one thing I want to do after a long day of work, and that's play a guitar.\n",
            "\n",
            "Related Content:\n",
            "\n",
            "No, It's Not All About the Pitch in Kevin Drew's No, It's Not All About the Pitch\n",
            "\n",
            "John Lennon: \"It's Not About ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-k-sampling & top-p-sampling combining both approachs"
      ],
      "metadata": {
        "id": "VVeByoxkTzKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#combine both sampling techniques\n",
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True,\n",
        "                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              #temperature = .7,\n",
        "                              top_k = 50,\n",
        "                              top_p = 0.85,\n",
        "                              num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqC8F1pwTucS",
        "outputId": "9cbaee2a-3a4f-4340-86d9-06aeb3f0e007"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: I don't know about you, but there's only one thing I want to do after a long day of work… and that's to take a break from the computer. Sure, I'll still use my tablet as well, but the keyboard is so nice that it's almost worth it. The keyboard on my X99 Pro is just so nice to type on. And even if I did get rid of the tablet, I'd still use it to keep track of what's going on in my system.\n",
            "\n",
            "The other thing I like about the keyboard on my X99 Pro is that the layout is super-easy to type on. The keys are so spaced that you'll be able to...\n",
            "\n",
            "1: I don't know about you, but there's only one thing I want to do after a long day of work: sit down and play some videogames with some friends. But if there's one thing I can say about my friend that's true, it's that we've all been together a long time, and we're still friends. But it's hard to stay friends with the same person for a whole lifetime, and I've been thinking about what my next game might be. And it has to do with a series of puzzles.\n",
            "\n",
            "A puzzle, or a game, in the sense of games that are not the same thing as puzzles. There are many, many games that you...\n",
            "\n",
            "2: I don't know about you, but there's only one thing I want to do after a long day of work.\"\n",
            "\n",
            "\"What is it, then?\"\n",
            "\n",
            "\"You were saying something about this book, weren't you?\"\n",
            "\n",
            "\"The book?\"\n",
            "\n",
            "\"Yeah. I know the title, and it sounds like something I'd like to read. You see, I've been thinking about it a lot, and I'm still trying to figure out what it's all about.\"\n",
            "\n",
            "\"Well, I'll tell you what I want to hear.\"\n",
            "\n",
            "\"Well, if you're talking about it, I'd love to know what it is. It's a...\n",
            "\n",
            "3: I don't know about you, but there's only one thing I want to do after a long day of work: go home, relax and watch the sunset....\n",
            "\n",
            "4: I don't know about you, but there's only one thing I want to do after a long day of work.\"\n",
            "\n",
            "Her smile had a certain look of resignation. \"Fine.\"\n",
            "\n",
            "\"Well, I guess I'll see you later. Good luck.\"\n",
            "\n",
            "She nodded, and walked away, leaving me alone with the door. I'd tried to look at her as she left, but I couldn't tell if she was looking away or not. I really couldn't tell.\n",
            "\n",
            "I went home and put the phone away.\n",
            "\n",
            "My phone rang. I answered it, and was surprised by the sound of my own voice. I got a message that there had been...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Benchmark Prompts"
      ],
      "metadata": {
        "id": "ueaHFLajURWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 150"
      ],
      "metadata": {
        "id": "yqQXPaQpUSNk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English."
      ],
      "metadata": {
        "id": "H8Ae2L7aVmG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt1 = 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'\n",
        "\n",
        "input_ids = tokenizer.encode(prompt1, return_tensors='tf')"
      ],
      "metadata": {
        "id": "J28uedvQUVDb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True,\n",
        "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              #temperature = .8,\n",
        "                              top_k = 50,\n",
        "                              top_p = 0.85\n",
        "                              #num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLG_Vf8CUakF",
        "outputId": "acf34445-4a4f-43cf-9349-2c4d3d04e0af"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
            "\n",
            "The researchers, led by biologist and professor of linguistics at Uppsala University, Anna-Kaisa Häggström, studied a group of wild unicorns called Gälltli, who are native to Argentina and Brazil. Their territory stretches from the foothills of the Andes Mountains in Peru and Argentina all the way up to the Amazon basin in Brazil.\n",
            "\n",
            "While most unicorns live in the mountains, the researchers found that the animals live in a valley near the town of Ch...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Giant Robot Found Beneath Metropolis, Sparks Debate Among Scientists."
      ],
      "metadata": {
        "id": "MwcizgTtVaWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can we use GPT-2 to generate fake news stories?\n",
        "prompt2 = 'Giant Robot Found Beneath Metropolis, Sparks Debate Among Scientists'\n",
        "\n",
        "input_ids = tokenizer.encode(prompt2, return_tensors='tf')"
      ],
      "metadata": {
        "id": "4vJGHrxLUh1f"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True,\n",
        "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              #temperature = .8,\n",
        "                              top_k = 50,\n",
        "                              top_p = 0.85\n",
        "                              #num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMG-MGu4UtHr",
        "outputId": "531d201b-c314-4c41-c2ef-8b2bab8df549"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Giant Robot Found Beneath Metropolis, Sparks Debate Among Scientists (VIDEO)\n",
            "\n",
            "The \"Giant Robot\" video is a spoof video created by the makers of \"Sharknado\" with a humorous twist.\n",
            "\n",
            "The video stars the famous actor Will Ferrell and features the voice of actor Stephen Baldwin, and features the famous phrase \"Giant Robot\" repeated multiple times.\n",
            "\n",
            "The clip, which is being shared online by a number of entertainment websites, was produced by the \"Sharknado\" filmmakers and features Ferrell as a scientist, and Baldwin as a scientist.\n",
            "\n",
            "The clip is being shared on Facebook and other social media sites, with many people praising the film and questioning whether the actors really are scientists...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Was Karna’s rivalry with Arjuna inevitable in Mahabharata?"
      ],
      "metadata": {
        "id": "wn-MlM5wUym4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can we use GPT-2 to imagine alternate histories of Lord of the Rings?\n",
        "\n",
        "prompt3 = 'Was Karna’s rivalry with Arjuna inevitable in Mahabharata?'\n",
        "\n",
        "input_ids = tokenizer.encode(prompt3, return_tensors='tf')"
      ],
      "metadata": {
        "id": "wFOx1dZsUzbD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True,\n",
        "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              #temperature = .8,\n",
        "                              top_k = 50,\n",
        "                              top_p = 0.85\n",
        "                              #num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85ooNrPOU3P2",
        "outputId": "2e0df3af-42f8-404e-8bbb-f7a074995a44"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Was Karna’s rivalry with Arjuna inevitable in Mahabharata?\n",
            "\n",
            "It is true that Karna was the first king of the Pandava kingdom and later on he was the head of the clan of Pandava kings in Arjuna's army. But this is not the first time that Arjuna and his army encountered Karna. In the Mahabharata, Karna's battle with Arjuna's army and its victory was already pre-planned. Karna's role was that of the spear-carrier who is in the front line, and Arjuna's role was that of the general who was to spearhead the attack.\n",
            "\n",
            "In this respect, Karna is a very modern...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For today’s homework assignment, please describe the reasons for the 1991 kargil war in india.\n",
        "\n"
      ],
      "metadata": {
        "id": "DqbBxgMfVBxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Can we use GPT-2 to do our homework?\n",
        "\n",
        "prompt4 = \"For today’s homework assignment, please describe the reasons for the 1991 kargil war in india.\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt4, return_tensors='tf')"
      ],
      "metadata": {
        "id": "sBbUG4euU-ko"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_outputs = GPT2.generate(\n",
        "                              input_ids,\n",
        "                              do_sample = True,\n",
        "                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n",
        "                              #temperature = .8,\n",
        "                              top_k = 50,\n",
        "                              top_p = 0.85\n",
        "                              #num_return_sequences = 5\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
        "    print('')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89t2yauxVLoh",
        "outputId": "8799d323-cc42-4858-8dac-c9914cae689f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: For today’s homework assignment, please describe the reasons for the 1991 kargil war in india.\n",
            "\n",
            "It is quite simple. We had been at war with Pakistan for over 20 years. In that time, over 2,000 people have lost their lives.\n",
            "\n",
            "This is the main reason why India had been hesitant in the past.\n",
            "\n",
            "We are still at war with Pakistan. This is also a reason why India needs to build a strong India-Pakistan alliance.\n",
            "\n",
            "As part of that, the country must take a stand on Kashmir, the problem that has created tension and discord between India and Pakistan.\n",
            "\n",
            "In the past, we have supported Kashmiri separatists. Now we must support those who want peace....\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYAr9_pEYQuH"
      },
      "execution_count": 48,
      "outputs": []
    }
  ]
}