
# Text Generation Using GPT-2

This project demonstrates how to use GPT-2 for text generation. It covers training GPT-2 on a custom dataset, fine-tuning the model, and generating new text sequences.

## Features
- Fine-tuning GPT-2 model using custom dataset
- Generating coherent text based on user prompts

## Installation
1. Clone the repository
2. Install the required dependencies from `requirements.txt`
3. Run the Jupyter notebook to start generating text

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
